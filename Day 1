To kick off the challenge, I read (once again) the foundational paper on Federated Learning by McMahan et al.: "Communication-Efficient Learning of Deep Networks from Decentralized Data." (https://lnkd.in/ePFG_PAb)

In this paper, the authors introduce the core concept of Federated Learning (FL), where multiple devices (called clients) collaboratively train a model with the help of a server while keeping their data local. They apply FL to three different types of models (multilayer perceptrons, CNNs, LSTMs) and consider a variety of tasks (e.g., digit recognition on the MNIST dataset and next-character prediction on a dataset of Shakespeare's plays).

The paper introduces the Federated Averaging (FedAvg) algorithm. In this approach, the server initially distributes a model to the clients, who then perform several steps of local training. After training, the clients send their locally trained models back to the server, which aggregates them into a global model by computing a weighted average. The server then shares the updated global model with the clients, and the process repeats iteratively until a predefined stopping criterion is met. This method enables the efficient aggregation of model updates from decentralized devices with minimal communication overhead. The authors demonstrate that it can produce high-quality models with a limited number of communication rounds across all the architectures considered.

What I find fascinating about this paper is how clearly the authors recognized they were opening a new research area. They also had an intuition about many of the directions this field would take, including challenges related to heterogeneity, scalability, and security.
